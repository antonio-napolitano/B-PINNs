{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/progetto_comp_stat"],"metadata":{"id":"c6zo73BHXQ58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n"],"metadata":{"id":"D2qf7cOnW8Ka"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrL3CGBnW23J"},"outputs":[],"source":["\n","\n","class MCD(tf.keras.Model):\n","    \"\"\"Monte Carlo dropout for UQ\"\"\"\n","\n","    def __init__(self, hidden_units=50, dropout_rate_u=0.005, dropout_rate_ode=0.001):\n","        super().__init__()\n","        self.denses = [\n","            tf.keras.layers.Dense(hidden_units, activation=tf.tanh),\n","            tf.keras.layers.Dense(hidden_units, activation=tf.tanh),\n","            tf.keras.layers.Dense(1),\n","        ]\n","        self.dropout_rate_u = dropout_rate_u\n","        self.dropout_rate_ode = dropout_rate_ode\n","\n","        self.log_mu = tf.Variable(tf.math.log(2.2), dtype=tf.float32)\n","        self.log_k = tf.Variable(tf.math.log(350.0), dtype=tf.float32)\n","        self.log_b = tf.Variable(tf.math.log(0.56), dtype=tf.float32)\n","\n","        self.opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","    def call(self, t, dropout_rate=0.0):\n","        dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n","        y = t\n","        for i in range(len(self.denses) - 1):\n","            y = self.denses[i](y)\n","            y = dropout_layer(y, training=True)\n","        return self.denses[-1](y)\n","\n","    def ODE(self, t, dropout_rate=0.0):\n","        with tf.GradientTape() as g_tt:\n","            g_tt.watch(t)\n","            with tf.GradientTape() as g_t:\n","                g_t.watch(t)\n","                x = self.call(t, dropout_rate=dropout_rate)\n","            x_t = g_t.gradient(x, t)\n","        x_tt = g_tt.gradient(x_t, t)\n","        return (\n","            1 / tf.exp(self.log_k) * x_tt\n","            + tf.exp(self.log_mu) / tf.exp(self.log_k) * x_t\n","            + (x - tf.exp(self.log_b))\n","        )\n","\n","    def train_op(self, t_u, x_u, t_ode):\n","        with tf.GradientTape() as tape:\n","            x_u_pred = self.call(t_u, dropout_rate=self.dropout_rate_u)\n","            f_ode = self.ODE(t_ode, dropout_rate=self.dropout_rate_ode)\n","            loss = tf.reduce_mean((x_u_pred - x_u) ** 2) + tf.reduce_mean(f_ode**2)\n","        grads = tape.gradient(loss, self.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n","        return loss\n","\n","    def train(self, t_u, x_u, t_ode, niter=10000):\n","        train_op = tf.function(self.train_op)\n","        loss = []\n","        for it in range(niter):\n","            loss_value = train_op(t_u, x_u, t_ode)\n","            loss += [loss_value.numpy()]\n","            if it % 100000 == 0:\n","                print(it, loss[-1])\n","        return loss\n","\n","    def infer(self, t, num_samples=1000):\n","        tt = tf.tile(t, [num_samples, 1])\n","        x = self.call(tt, dropout_rate=self.dropout_rate_u)\n","        return tf.reshape(x, [num_samples, t.shape[0], -1])"]}]}