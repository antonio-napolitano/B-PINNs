{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mHedrBeAOPrh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709804250484,"user_tz":-60,"elapsed":2028,"user":{"displayName":"Antonio Napolitano","userId":"09394917420548624620"}},"outputId":"97149229-dd52-471f-ee9e-a4e3adff0c3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/MyDrive/progetto_comp_stat\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/progetto_comp_stat"]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_probability as tfp"],"metadata":{"id":"enpBxScUUfgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","tfd = tfp.distributions\n","\n","\n","class PI_Bayesian:\n","    def __init__(\n","        self,\n","        x_u,\n","        y_u,\n","        x_pde,\n","        pde_fn,\n","        L=6,\n","        noise_u=0.05,\n","        noise_pde=0.05,\n","        prior_sigma=1.0,\n","    ):\n","        self.x_u = x_u\n","        self.y_u = y_u\n","        self.x_pde = x_pde\n","\n","        self.pde_fn = pde_fn\n","        self.L = L\n","        self.noise_u = noise_u\n","        self.noise_pde = noise_pde\n","        self.prior_sigma = prior_sigma\n","\n","        self.log_mu_init = tf.math.log(2.2)\n","        self.log_k_init = tf.math.log(350.0)\n","        #self.log_b_init = tf.math.log(0.56)\n","\n","        # self.log_mu_init = tf.math.log(-5.0)\n","        # self.log_k_init = tf.math.log(1.0)\n","        # self.log_b_init = tf.math.log(0.56)\n","\n","        self.additional_inits = [self.log_mu_init, self.log_k_init] #, self.log_b_init\n","        self.additional_priors = [\n","            tfd.Normal(0, scale=0.5),\n","            tfd.Normal(0, scale=0.5),\n","\n","        ] #tfd.Normal(0, scale=0.5),\n","\n","    def build_posterior(self, bnn_fn):\n","        y_u = tf.constant(self.y_u, dtype=tf.float32)\n","\n","        def _fn(*variables):\n","            \"\"\"\n","            log posterior function, which takes neural network's parameters input, and outputs (probably unnormalized) density probability\n","            \"\"\"\n","            # split the input list into variables for neural networks, and additional variables\n","            variables_nn = variables[: 2 * self.L]\n","            log_mu, log_k = variables[2 * self.L :] #, log_b\n","            mu, k = (\n","                tf.exp(log_mu + self.log_mu_init),\n","                tf.exp(log_k + self.log_k_init),\n","\n","            ) #, b  tf.exp(log_b + self.log_b_init),\n","            # explicitly create a tf.Tensor here, for input to neural networks, to avoid bugs\n","            x_u = tf.constant(self.x_u, dtype=tf.float32)\n","            x_pde = tf.constant(self.x_pde, dtype=tf.float32)\n","\n","            # make inference\n","            _fn = lambda x: bnn_fn(x, variables_nn)\n","            y_u_pred = _fn(x_u)\n","            pde_pred = self.pde_fn(x_pde, _fn, [mu, k]) #, b\n","\n","            # construct prior distributions, likelihood distributions\n","            u_likeli = tfd.Normal(loc=y_u, scale=self.noise_u * tf.ones_like(y_u))\n","            noise_pde1, noise_pde2 = self.noise_pde\n","            N1, N2 = y_u_pred.shape[0], pde_pred.shape[0]\n","            pde_likeli_1 = tfd.Normal(\n","                loc=tf.zeros([N1, 1]), scale=noise_pde1 * tf.ones([N1, 1])\n","            )\n","            pde_likeli_2 = tfd.Normal(\n","                loc=tf.zeros([N2 - N1, 1]), scale=noise_pde2 * tf.ones([N2 - N1, 1])\n","            )\n","            # pde_likeli = tfd.Normal(loc=tf.zeros_like(pde_pred), scale=self.noise_pde*tf.ones_like(pde_pred))\n","\n","            prior = tfd.Normal(loc=0, scale=self.prior_sigma)\n","\n","            # compute unnormalized log posterior, by adding log prior and log likelihood\n","            log_prior = tf.reduce_sum(\n","                [tf.reduce_sum(prior.log_prob(var)) for var in variables_nn]\n","            ) + tf.reduce_sum(\n","                [\n","                    dist.log_prob(v)\n","                    for v, dist in zip([log_mu, log_k], self.additional_priors)  #, log_b\n","                ]\n","            )\n","            # log_prior += tf.reduce_sum([dist.log_prob(v) for v, dist in zip([(mu-2.2)/2.2, (k-370.0)/370.0, (b-0.56)/0.56], self.additional_priors)])\n","            log_likeli = (\n","                tf.reduce_sum(u_likeli.log_prob(y_u_pred))\n","                + tf.reduce_sum(pde_likeli_1.log_prob(pde_pred[:N1, :]))\n","                + tf.reduce_sum(pde_likeli_2.log_prob(pde_pred[N1:N2, :]))\n","            )\n","            return log_prior + log_likeli\n","\n","        return _fn"],"metadata":{"id":"cvaNBm8BRsA5"},"execution_count":null,"outputs":[]}]}